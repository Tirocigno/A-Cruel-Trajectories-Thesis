Apache Spark è un framework per il calcolo distribuito e general purpouse.
Spark nasce dalle trasformazioni che gli hardware e i software hanno subito nel corso del tempo e dalle nuove esigenze emerse.
Negli ultimi anni sono disponibili macchine con CPU aventi sempre più core.
Oltre a questo è stato registrato anche un aumento della memoria ram nelle macchine.
Questo incremento ha permesso alla ram di divenire la memoria principale durante le computazioni, sostituendo il disco anche nella gestione di grandi moli di dati.
Dal punto di vista del software invece, è avvenuta un evoluzione che ha dato maggior rilevanza ai paradigmi funzionali rispetto a quelli ad oggetti.
Allo stesso modo i sistemi NoSQL, orientati a velocità e disponibilità hanno messo in ombra i classici sistemi SQL, orientati alla consistenza del dato.
Anche nel mondo Big Data sono emerse nuove possibilità ed esigenze: la necessità di poter processare dati in streaming, favorire approcci veloci e l'integrazione
con nuovi ambiti della data science, come ad esempio il machine learning sono alcuni esempi.

Alla luce di questi cambiamenti, Map-Reduce è divenuto limitante.
Questo paradigma infatti non si presta a tutti i tipi di elaborazione, essendo pensato per un approccio batch e basato sul disco.
Spark si pone come integrazione di Map-Reduce a queste nuove esigenze.
È in grado infatti di eseguire sia processi batch che streaming o query interattive.
Le sue primitive, basate sull'accesso a dati memorizzati in RAM, consentono performance cento volte più veloci di Map-Reduce.
Spark inoltre mantiene l'assoluta compatibilità con Hadoop tramite YARN e supporta varie fonti di dati, come ad esempio Hive.