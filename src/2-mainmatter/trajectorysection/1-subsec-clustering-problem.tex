Il clustering, o analisi di raggruppamento, è una tecnica con lo scopo di aggregare i dati in \textit{cluster}, o gruppi,
tali che i dati all'interno di un gruppo siano più simili tra loro rispetto a quelli all'esterno~\cite*{liao2005clustering, zazzarro2009clustering}.
Questa tecnica è definita come non supervisionata, ovvero non è necessario avere una conoscenza
sui gruppi presenti nel dataset.

Le principali categorie di clustering per elaborare dati statici a bassa dimensionalità sono cinque~\cite{han2011data}:
metodi basati sulle partizioni, sulla gerarchia, densità, griglia e infine modello.


\paragraph{Metodi basati sulle partizioni}
Il clustering partizionale consiste nell'individuare \textit{k} punti e successivamente
eseguire l'assegnazione di ogni elemento del dataset a uno di questi.
In tal modo sono individuate \textit{k} partizioni, che corrispondono ad altrettanti cluster.
Questo metodo ha come vantaggio la semplicità e il ridotto numero di iperparametri,
è infatti necessario specificare a priori solo il valore di \textit{k}.
Due esempi di algoritmi~\cite{arora2016analysis} possono essere \textit{k-means} e \textit{k-medioid}:
entrambi partono dall'individuare \textit{k} punti a caso all'interno del dataset, successivamente eseguono l'assegnazione dei restanti elementi,
calcolano un nuovo centro del cluster in un caso, un medioide (punto che minimizza la dissimilarità rispetto agli altri elementi del cluster) nell'altro e ripetono le operazioni fino a giungere a una convergenza ai centri dei cluster.
In figura~\cref{fig:chap-1:k-means-overview} è possibile vedere il risultato di k-means su un dataset fissato.
I limiti di questo approccio sono l'eccessiva rigidità delle regole applicate nella generazione dei cluster e la necessità di conoscere a prescindere il loro numero (\textit{k}).

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{/sec-1/KMeansNew.pdf}
  \caption{Cluster individuati da  k-means, Fonte:~\cite{KMeansDa64:online}}%
  \label{fig:chap-1:k-means-overview}
\end{figure}

\paragraph{Metodi basati sulla gerarchia}
Passando ai metodi basati sulla gerarchia, questi prevedono una divisione del dataset definendo un criterio gerarchico tra i vari elementi.
A seconda della modalità di divisione si possono individuare due categorie: se la gerarchia è definita combinando di volta in volta i singoli elementi,
allora si parla di clustering gerarchico agglomerativo.
Nel caso la ricerca avvenga dividendo ad ogni passo i cluster in sottoinsiemi, invece
si tratta di clustering gerarchico divisivo.
Gli algoritmi agglomerativi adottano un approccio \textit{bottom-up}: partono considerando ogni punto come un gruppo ed a ogni passo fondono i cluster sulla base di determinate metriche
di similarità. Tale passaggio viene ripetuto fino al soddisfacimento di una condizione.
Al contrario gli algoritmi divisivi partono da un unico cluster, contenente tutti i punti del dataset, e dividono di volta
in volta i cluster in cluster più piccoli.
\textit{Agglomerative Nesting (AGNES)}~\cite{kaufman2008agglomerative} e \textit{Divisive Analysis (DIANA)}~\cite{kaufman2008divisive} sono due algoritmi che
appartengono rispettivamente al clustering agglomerativo e a quello divisivo.
Gli algoritmi di clustering gerarchico hanno il vantaggio di essere semplici, tuttavia risulta
difficile definire i punti di divisione o aggregazione dei cluster.
In più la natura irreversibile delle divisioni/aggregazioni rende complicato giungere a soluzione ottime.

\paragraph{Metodi basati sulla densità}
La categoria di algoritmi basati sulla densità ha alla base un'idea diversa rispetto a quanto detto fin d'ora.
Data un'area spaziale, la cui densità è sopra una soglia di rilevanza, e un insieme di cluster, si assegnano i punti in questione al cluster più vicino.
Il clustering basato sulla densità riesce così ad individuare raggruppamenti di ogni forma e dimensione.
Due algoritmi emblematici della categoria sono \textit{DBSCAN}~\cite{ester1996density} e \textit{OPTICS}~\cite{ankerst1999optics}.

\textit{Density-Based Spatial Clustering of Applications with Noise}, o \textit{DBSCAN}, è un algoritmo
di clustering che permette di individuare cluster di ogni forma e dimensione grazie alla connessione basata sulla densità dei singoli cluster.
Dato un set di punti, questi vengono divisi in core point, density-reachable point e outlier secondo il seguente criterio:
Il punto \(p\) si definisce core point se esistono almeno \(minPts\) punti a distanza minore o uguale di
\(~\epsilon \), questi sono poi etichettati come direttamente raggiungibili da \(p\).
Un punto \textit{q} è definito raggiungibile da \(p\) se esiste un'insieme di punti
\(p_{1},\ldots, p_{n}\) dove \(p_{1} = p \) e
\(p_{n} = q \) dove ogni \(p_{i + 1}\) è direttamente raggiungibile da
\(p_{i}\), quindi tutti i punti all'interno della sequenza sono core.
Un punto non raggiungibile da nessun altro viene definito outlier.
Se \textit{p} è un core point, allora genera un cluster assieme a tutti i punto raggiungibili.
Tutti i punti all'interno di un cluster devono mantenere una densità di almeno \(minPts\) punti all'interno del proprio vicinato di raggio~\(~\epsilon \).
I valori dei parametri~\(~\epsilon \) e \(minPts\) sono decisi senza una regola precisa, sta infatti all'esperienza dell'utente stabilire valori sensati e in questo risiede la maggiore debolezza di DBSCAN\@.
In~\cref*{fig:chap-1:dbscan-points} è possibile vedere 
il risultato dell'algoritmo su un dataset avente cluster
di forma non regolari.

\begin{figure}
  \centering
  \includegraphics[scale=.5]{/sec-1/DBSCAN.pdf}
  \caption{Cluster individuati da DBSCAN, Fonte:~\cite{DBSCANWh86:online}}%
  \label{fig:chap-1:dbscan-points}
\end{figure}

\paragraph{Metodi basati sulla griglia}
Approccio ancora differente è quello dei metodi basati su griglia, che impiegano una griglia multi-risoluzione per suddividere lo spazio in un limitato numero di celle.
Su ognuna di queste celle vengono eseguite in parallelo le operazioni di clustering, i cui risultati sono direttamente collegati alla compressione
effettuata dalle dimensioni della singola cella.
Il vantaggio principale di questa metodologia è la totale indipendenza dalle dimensioni del dataset: solo il numero di
celle su ogni dimensione influenza il tempo di computazione.
Un esempio di questa famiglia di algoritmi è \textit{STING}~\cite{wang1997sting},
che divide lo spazio di ricerca in rettangoli ad alta dimensionalità, scomponendo poi ciascuno di questi
in strutture a minore dimensionalità e conservando per ciascun livello un insieme di informazioni statistiche sugli attributi.

\paragraph{Metodi basati sul modello}
Ultima categoria trattata è quella degli algoritmi basati su modello.
Questi assumono
che per ogni cluster sia presente un modello matematico che lo descriva e ricercano l'insieme dei punti che meglio si adattano
al modello selezionato.
In primo luogo vengono calcolate le funzioni di densità che riflettono la distribuzione dei dati,
successivamente tenta di adattare la distribuzione dei dati a un certo modello matematico. \textit{COBWEB}~\cite{fisher1987knowledge} è
un esempio di algoritmo che ricerca raggruppamenti sui dati tramite una gerarchia di concetti.
Il clustering basato su modelli molto spesso fatica nel trovare la descrizione migliore per le relazioni tra i diversi punti del dataset.





